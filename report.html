<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Valorant Interactive Analysis – Final Report</title>
</head>
<body style="margin: 40px; font-family: Times New Roman, sans-serif; line-height: 1.6;">

<h1>Valorant Interactive Analysis – Final Report</h1>

<p> Interactive Analysis of Valorant Esports Data

Group: Haosen Zhong, Fanyun Yang
Project Link: https://haos3n.github.io/Valorant-Match-Interactive-Analysis/index.html
Introduction
This project explores the competitive landscape of Valorant esports from 2021 to 2025 through a web-based interactive visualization system. Rather than producing one static dashboard, we designed and implemented a multi-view website that allows users to move fluidly between player-level, agent-level, team-level and match-level perspectives. Our goal was not only to summarize basic statistics, but to give users tools to ask their own questions about performance, strategy and meta evolution across several years of professional play.
Project Goals, User Personas, and Analytical Questions
From the beginning, we framed the project around three main user personas. The first persona is the professional or semi-professional analyst or coach, who wants to understand how particular players, teams and compositions perform across tournaments and maps. This user needs to identify trends such as whether a player’s rating is consistent across different roles, which agents overperform on specific maps, and how a team’s map pool has changed over time. The second persona is the engaged esports fan or content creator, who is less concerned with formal statistics but very interested in exploring standout performances, iconic matches and meta shifts. This user wants to answer questions like which players dominated a particular year, which agents suddenly became popular, or how a favorite team performed in a specific tournament. The third persona is a data-science-oriented student or researcher, who treats the dataset as a real-world example of complex, event-level sports data and is interested in relationships between performance metrics, strategies and outcomes.
To support these personas, we formulated a set of analytical questions that our system should enable. At the player level, users should be able to ask how individual performance metrics such as rating, average combat score (ACS), average damage per round (ADR), kill/death ratio and headshot percentage vary across years, maps, teams and tournaments. At the agent level, users should be able to see which agents are most frequently picked, which agents yield the highest win rates on different maps, and how the agent meta shifts over time. At the team level, users should be able to analyze how teams perform on different maps, how successful particular agent compositions are, and whether certain teams rely heavily on specific strategies. At the match level, users should be able to explore eco rounds, kill patterns and round-to-round momentum to understand how individual games developed rather than only looking at final scores. The overarching goal was to build a system where these questions could be explored interactively rather than answered only through fixed, pre-written summaries.
Input Data
The input data consists of a large multi-year dataset of Valorant Champions Tour matches from 2021 to 2025. The data is distributed across several folders that correspond to different analytical layers. One group of files describes agents and their usage, including pick rates, map statistics and team-agent combinations. Another group identifies entities such as players, teams, tournaments and stages. A third group captures detailed match data, including kill logs, eco rounds, map scores, round outcomes and overall match summaries. Finally, a player statistics file aggregates per-player metrics such as rating, ACS, ADR, kills per round and headshot percentage.
The scale and structure of the raw data presented both opportunities and constraints. On the positive side, the dataset covers multiple competitive seasons, a large number of teams and players, and very fine-grained event information for individual rounds and kills. This richness made it possible to design views that go far beyond simple win–loss summaries. On the other hand, the files were often very large, with some exceeding tens of megabytes, and encoding formats were not always consistent. There were also gaps or inconsistencies across years and tournaments, especially in the earliest seasons. As a result, the data recommended itself for interactive exploration but required substantial preprocessing before it could be served through a browser-based visualization.
Intended Visualization Designs: Data, Visual and Interaction
In our initial proposal we designed the system around four interconnected views, each tied to a specific analytical focus but sharing a consistent dark theme and visual language inspired by Valorant’s own interface aesthetics. The player view was intended to center on scatterplots that used rating, ACS and ADR as axes, with additional channels such as color and size to encode team or tournament. This view would be complemented by a sortable table so that users could quickly move between overview and detail. Interaction in this view was envisioned as a combination of filtering (by year, team or player) and brushing (highlighting a subset of points and seeing them reflected in the table).
The agent view was designed around the idea of a “meta map” in which agents would be represented as bubbles positioned by pick rate and win rate and colored by role or map. Users would be able to select a particular map and immediately see which agents are overrepresented or unusually successful, supporting questions about meta viability. We also planned a supplementary map-level panel that would display trends across years for a chosen agent. For teams, our proposed design combined a map win-rate grid with small multiples of agent compositions, allowing users to see, for example, whether a team’s success on a map was tied to a specific composition or whether that team was more flexible. The match view was intended to visualize eco rounds and kill sequences, using timelines and stacked bars to show how economy decisions and early kills influenced the course of a game. Across all views, our interaction design emphasized smooth transitions, hover tooltips and click-to-filter behavior so that users could move from global trends to specific examples without losing context.
These design choices were directly linked to the analytical questions we wanted to support. Scatterplots with sortable tables are well suited to comparing players on multiple metrics simultaneously. Bubble charts with two performance axes naturally represent trade-offs in agent usage and success. Grids and timelines capture team map pools and match flow over time. By keeping visual encodings consistent (for example, using the same color mapping for teams across views), we aimed to reduce cognitive load and make it easier for users to mentally connect patterns observed in different parts of the system.
Data Transformation and Statistical Analysis
To make these visualizations feasible and responsive in the browser, we carried out several stages of data transformation and analysis. All preprocessing was done in Python using Pandas. First, we standardized the character encodings across files; many CSVs were stored with Latin-1 or Windows code pages rather than UTF-8, which initially caused decoding errors. We wrote small scripts that attempted multiple encodings and converted everything to UTF-8 once a file could be read successfully. Second, we normalized key identifiers so that player names, team names and agent labels matched across files and years, which was crucial for merging separate tables into coherent analytic datasets.
Because some raw files were too large to host directly on GitHub Pages, we made a pragmatic compromise by retaining only the columns that were needed for visualization and trimming each file to an upper bound on rows (for example, keeping the first 80,000 records). This step necessarily removes some granularity but preserves overall patterns and ensures that the web application loads quickly even with multiple datasets. We then derived higher-level metrics used in the visualizations. At the player level, we computed and stored rating, ACS, ADR, kill/death ratio and headshot percentage in a single table per year, rather than recalculating them on the fly. At the agent level, we aggregated match- and round-level data into pick and win rates by agent and map. At the team level, we computed win rates by map and agent composition, and we summarized eco-round and kill-sequence data to provide match-level indicators such as eco win rate.
In several places we had to balance statistical precision with practical constraints. For example, fully modeling round-by-round win probability would require more advanced time-series modeling and might not be robust across uneven data quality in earlier years. Instead, we chose to focus on interpretable summary metrics that map cleanly onto visual encodings: rates, averages and ratios that can be shown as positions, colors or sizes. These decisions kept the system transparent and aligned with the mental models of our target users, who often prefer clear, interpretable statistics over opaque black-box models.
Final Design and Implementation (Prototype)
By the end of the semester, we implemented the system as a multi-page website hosted on GitHub Pages. The entry point is index.html, which introduces the project and provides navigation to four main analytical views: agents, players, teams and matches. Each of these views is implemented as its own HTML file (agents.html, player.html, team.html, and match.html) that loads the relevant CSV data using D3.js and renders interactive visualizations directly in the browser.
The player page presents a coordinated set of components: a scatterplot that plots rating against ACS or ADR, a set of filters for year, team and player, and a data table that lists player statistics. The table supports sorting by rating, K/D, ACS, headshot percentage and ADR, allowing users to rank players according to different criteria. Clicking on a point in the scatterplot filters the table to that specific player, while changing filters updates both the chart and the table. The agents page displays agents as bubbles positioned by pick rate and win rate on selected maps, with color indicating agent class. Users can switch maps and years and immediately see how the meta shifts. The team page focuses on map performance and composition; it shows map win rates for selected teams and updates supporting charts when a team or map is clicked, correcting earlier issues such as win rates exceeding 100%. The match page visualizes eco statistics and kill patterns, offering a match-centric lens that complements the more aggregate views.
Across all pages, the implementation emphasizes consistency and responsiveness. We use a dark background with accent colors inspired by Valorant’s UI, and interactive elements such as hover tooltips and click selections follow similar patterns throughout the site. Transitions are subtle but help users understand that they are seeing the same data filtered or re-ordered, rather than entirely new views. Although the final implementation does not include every idea from the original proposal—for example, some more advanced kill-sequence animations were simplified to preserve performance—it captures the core intention of a multi-view, interactive analysis environment spanning players, agents, teams and matches over five competitive seasons.
How the Final Design Supports Analytical Questions
To evaluate how well the final design supports the intended analytical questions, it is helpful to walk through a few example usage scenarios. Suppose a coach wants to identify the most reliable duelists for a certain year. In the player view, they can filter to that year, sort the table by rating or ACS, and immediately see which players rank at the top. Clicking on a particular player highlights that point in the scatterplot and reveals the tournaments and teams associated with that performance. If the coach suspects that a player’s success comes from a specific agent or role, they can switch to the agent view and examine which agents have high win rates on the relevant maps, then cross-reference those with the player’s known preferences.
As another example, consider a fan who wants to understand why a specific team, such as Paper Rex, is considered strong on a particular map. In the team view, they can select Paper Rex and the map of interest and see a corrected win rate that accounts for wins and losses rather than raw match counts. The view then shows which agents the team tended to pick on that map and how successful those compositions were. By clicking through different maps, the fan can see whether the team is a specialist on a small map pool or more broadly flexible. If they want to drill into individual games, they can move to the match view and explore eco rounds and kill statistics to see whether the team relies on aggressive force buys, disciplined saves or strong pistol rounds.
These scenarios illustrate that the system does not merely provide static charts but actually enables users to ask and answer their own questions. Filtering, sorting and cross-view consistency make it possible to move from high-level summaries to specific cases and back again. While the system is not exhaustive—it cannot model every subtle aspect of professional play—it provides a practical and accessible way for different user personas to engage with a complex esports dataset.
Design and Development Process Reflection
Looking back at the development process, our project followed the broad outline of the original proposal but required several adjustments as we encountered technical and data-related constraints. Initially, we imagined even more sophisticated interactions, such as fully animated kill timelines and highly detailed round-by-round replays. As we worked with the data and tested performance in the browser, it became clear that these ideas would be difficult to implement within the time frame and hosting limitations. This led us to prioritize stable, interpretable views over more experimental but fragile features.
Some parts of the project turned out to be more time-consuming than expected. Data cleaning and preprocessing took a significant amount of effort, especially when dealing with inconsistent encodings, very large files and year-to-year schema variations. Hosting constraints forced us to think carefully about file sizes and led to the trimming and aggregation steps described earlier. On the other hand, once the data had been standardized, building coordinated D3 views became more straightforward, and adding interactions such as sorting, filtering and click-based highlighting was easier than initially feared. A recurring theme was the need to balance ambition with robustness: we gradually refined the scope to focus on features that could be fully implemented and debugged rather than spreading effort across too many partially working ideas.
Reaching the end of the semester, one key reflection is that good visualization design is tied as much to data and infrastructure realities as it is to visual creativity. The final system is different in detail from the original sketches, but it remains faithful to the core goal of enabling multi-perspective exploration of Valorant esports performance. If we had more time, future work could include deeper temporal modeling of match momentum, richer integration across views, and features such as saving user-defined filters or exporting customized reports. Nonetheless, the current prototype already demonstrates a complete design loop from initial problem definition through data preparation, visualization design and functioning interactive implementation.
Ethical and Societal Considerations
One consideration is fairness and representation. Competitive data often reflects structural differences between regions, organizations and players with different levels of support. Our system does not attempt to adjust for these factors; it treats all matches as equal units of analysis. As a result, teams and players from regions with more tournaments or better infrastructure may appear more frequently and therefore dominate impressions of the scene. Users should be aware that the visualizations reflect observed performance within a particular competitive ecosystem and do not capture the broader context of access to resources or opportunities. In a more advanced version of the project, it would be interesting to design views that explicitly show representation imbalances across regions, genders or leagues.
A final issue is the potential impact of analytic tools on the esports ecosystem itself. Detailed public analytics can empower coaches, analysts and fans, but they can also contribute to increased pressure on players and to an arms race of data-driven optimization that makes the environment more stressful and less accessible. Our project is small in scale and intended primarily for educational purposes, yet it is built on the same logic as professional analytics platforms. Recognizing this, we view the dashboard as an opportunity to think critically about how data is used in competitive environments and to encourage responsible interpretation rather than purely exploitative optimization. Overall, while the ethical stakes here are lower than in domains such as healthcare or policing, incorporating this reflective perspective helped us design the system with more awareness and care.
</p>

</body>
</html>
